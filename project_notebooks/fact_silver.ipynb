{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89e8f290-dfad-48a7-8f2c-1da7513c4edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when, col, lit, current_timestamp,row_number, expr\n",
    "from pyspark.sql.functions import split\n",
    "import os\n",
    "from pyspark.sql.functions import col, crc32\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, TimestampType\n",
    "from pyspark.sql.functions import col, concat, year, month, dayofmonth\n",
    "import subprocess\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c79d0321-3688-4b98-8a27-e7d0c291d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .appName(\"fact\")\\\n",
    "    .config(\"spark.eventLog.logBlockUpdates.enabled\", True)\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c730d6e-99e0-4986-96a8-2f872d0ae3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.repl.eagerEval.maxColWidth\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a8d2373-7894-4dfe-8219-582dc3f427b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_lateset_file(directory):\n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "    list_status = fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(directory))\n",
    "    files = [(file.getPath().toString(), file.getModificationTime()) for file in list_status if file.isFile()]\n",
    "    files.sort(key=lambda x: x[1], reverse=True)\n",
    "    latest_file = files[0][0] if files else None\n",
    "    return latest_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d97aabd5-0c7b-4e50-ba9d-3edabce4d41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://localhost:9000/user/sales_transaction/sales_transactions_SS_raw_2.csv\n"
     ]
    }
   ],
   "source": [
    "latest_file = get_lateset_file('/user/sales_transaction/')\n",
    "print(latest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52027d9f-abb4-441a-a046-454933556810",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_checkpoint(checkpoint_path, latest_file):\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    result = subprocess.run(['hdfs', 'dfs', '-test', '-e', checkpoint_dir])\n",
    "    if result.returncode != 0:\n",
    "        subprocess.run(['hdfs', 'dfs', '-mkdir', '-p', checkpoint_dir])\n",
    "    \n",
    "    with open('/tmp/checkpoint_tmp.txt', 'w') as f:\n",
    "        f.write(latest_file)\n",
    "    \n",
    "    subprocess.run(['hdfs', 'dfs', '-put', '-f', '/tmp/checkpoint_tmp.txt', checkpoint_path])\n",
    "    os.remove('/tmp/checkpoint_tmp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77081f63-61e1-42af-b40e-9700016f49d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_last_sur(table_name, column_name):\n",
    "    \n",
    "    query = f\"SELECT max({column_name}) as last_sur_key FROM {table_name}\"\n",
    "    df = spark.sql(query)\n",
    "    max_sur_key = df.select(\"last_sur_key\").first()[0] if df.count() > 0 else 0\n",
    "    \n",
    "    return max_sur_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49f6cf4e-3186-4327-a2fd-79a66e9182ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_last_sur(table_name, column_name, last_sur):\n",
    "    schema = StructType([StructField(column_name, LongType(), False)])\n",
    "    data = [(last_sur,)]\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    df.write.mode(\"overwrite\").saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb307ec-b1fc-4253-9ed2-87c098b2fb91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c3edd7a-74d8-4071-8562-52278668df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"transaction_date\", TimestampType(), True),\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_fname\", StringType(), True),\n",
    "    StructField(\"cusomter_lname\", StringType(), True),\n",
    "    StructField(\"cusomter_email\", StringType(), True),\n",
    "    StructField(\"sales_agent_id\", IntegerType(), True),\n",
    "    StructField(\"branch_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"product_category\", StringType(), True),\n",
    "    StructField(\"offer_1\", BooleanType(), True),\n",
    "    StructField(\"offer_2\", BooleanType(), True),\n",
    "    StructField(\"offer_3\", BooleanType(), True),\n",
    "    StructField(\"offer_4\", BooleanType(), True),\n",
    "    StructField(\"offer_5\", BooleanType(), True),\n",
    "    StructField(\"units\", IntegerType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"is_online\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"shipping_address\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "913b6a10-53af-4c85-9961-c9018681f79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_audit_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS audit_dim (\n",
    "    audit_sur BIGINT\n",
    ")\n",
    "\"\"\"\n",
    "spark.sql(create_audit_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13f01b03-c6a8-408e-a830-8370a5061caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash(df, column_name):\n",
    "    if column_name:\n",
    "        return df.withColumn(\"customer_sur\", crc32(col(column_name)))\n",
    "    else:\n",
    "        return df.withColumn(\"customer_sur\", crc32(lit(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e99a5cb5-94e6-4ba9-9a91-d1b77eae924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_audit_dimension(df_transactions, audit_dim, column_id):\n",
    "    last_audit_id = fetch_last_sur(\"audit_dim\", \"audit_sur\")\n",
    "    if last_audit_id is None:\n",
    "        last_audit_id = 0  \n",
    "    \n",
    "    windowSpec = Window.orderBy(\"transaction_date\")\n",
    "    df_transactions = df_transactions.withColumn(\"row_num\", row_number().over(windowSpec))\n",
    "    df_transactions = df_transactions.withColumn(\"audit_sur\", last_audit_id + col(\"row_num\"))\n",
    "    \n",
    "    df_transactions = df_transactions.withColumn(\"source_file\", lit(latest_file))\n",
    "    df_transactions = df_transactions.withColumn(\"created_at\", current_timestamp())\n",
    "    df_transactions = df_transactions.withColumn(\"created_by\", lit(\"Eslam Fayez\"))\n",
    "    df_transactions = df_transactions.withColumn(\"is_valid_email\", expr(\"customer_email like '%@%.%'\"))\n",
    "    df_transactions = df_transactions.withColumn(\"is_positive_units\", col(\"units\") > 0)\n",
    "    df_transactions = df_transactions.withColumn(\"is_positive_unit_price\", col(\"unit_price\") > 0)\n",
    "    df_transactions = df_transactions.withColumn(\"is_valid_transaction\", col(\"is_valid_email\") & col(\"is_positive_units\") & col(\"is_positive_unit_price\"))\n",
    "    \n",
    "    return df_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb0841f6-f694-44cd-91f3-31b0ad5a3c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_date(df, date_col):\n",
    "    return df.withColumn(\"date_sur\", \n",
    "                         concat(year(col(date_col)).cast(\"string\"), \n",
    "                                month(col(date_col)).cast(\"string\"), \n",
    "                                dayofmonth(col(date_col)).cast(\"string\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ced5ca10-2c1c-4fa5-9ff2-6610a3830fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://localhost:9000/user/sales_transaction/sales_transactions_SS_raw_1.csv\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"/user/checkpoint/checkpoint_transactions.txt\"\n",
    "result = subprocess.run(['hdfs', 'dfs', '-test', '-e', checkpoint_path])\n",
    "if result.returncode != 0:\n",
    "    latest_processed_file = \"\"\n",
    "else:\n",
    "    rdd = sc.textFile(\"/user/checkpoint/checkpoint_transactions.txt\")\n",
    "    latest_processed_file = rdd.take(rdd.count())[-1]\n",
    "    print(latest_processed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7961788-d686-4d91-b7e9-6a98715cc4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists in HDFS: /user/silver/sales_transaction/sales_transactions.parquet\n"
     ]
    }
   ],
   "source": [
    "processed_path = \"/user/silver/sales_transaction/sales_transactions.parquet\"\n",
    "processed_dir = \"/user/silver/sales_transaction\"\n",
    "checkpoint_path = \"/user/checkpoint/checkpoint_transactions.txt\"\n",
    "\n",
    "latest_file = get_lateset_file('/user/sales_transaction/')\n",
    "\n",
    "\n",
    "\n",
    "if latest_processed_file == latest_file:\n",
    "    print(f\"File already processed before: {latest_processed_file}\")\n",
    "    sc.stop()\n",
    "else:\n",
    "    df_transactions = spark.read.csv(latest_file, header=True, schema=schema)    \n",
    "    \n",
    "    df_transactions = df_transactions.withColumn(\"customer_email\", F.regexp_replace(\"cusomter_email\", r\"\\.com.*\", \".com\"))\n",
    "    df_transactions = df_transactions.withColumnRenamed(\"cusomter_lname\", \"customer_lname\")\n",
    "    df_transactions = df_transactions.withColumn(\n",
    "        \"offers\",\n",
    "        when(col(\"offer_1\") == \"TRUE\", 5)\n",
    "        .when(col(\"offer_2\") == \"TRUE\", 10)\n",
    "        .when(col(\"offer_3\") == \"TRUE\", 15)\n",
    "        .when(col(\"offer_4\") == \"TRUE\", 20)\n",
    "        .when(col(\"offer_5\") == \"TRUE\", 25)\n",
    "        .otherwise(\"0\")\n",
    "    )\n",
    "    df_transactions = df_transactions.withColumn(\"total_price\", col(\"units\") * col(\"unit_price\"))\n",
    "    df_transactions = df_transactions.withColumn('address_split', split(df_transactions['shipping_address'], '/'))\n",
    "    df_transactions = df_transactions.withColumn('shipping_address', df_transactions['address_split'].getItem(0)) \\\n",
    "    .withColumn('city', df_transactions['address_split'].getItem(1)) \\\n",
    "    .withColumn('state', df_transactions['address_split'].getItem(2)) \\\n",
    "    .withColumn('postal_code', df_transactions['address_split'].getItem(3))\n",
    "    df_transactions = df_transactions.drop('address_split',\"offer_1\",\"offer_2\",\"offer_3\",\"offer_4\",\"offer_5\",\"row_num\",\"cusomter_email\")\n",
    "    df_transactions = create_audit_dimension(df_transactions, \"audit_dim\", \"audit_sur\")\n",
    "    df_transactions = get_hash(df_transactions,\"customer_email\")\n",
    "    df_transactions = concat_date(df_transactions,\"transaction_date\")\n",
    "    #df_transactions = df_transactions.drop('address_split',\"offer_1\",\"offer_2\",\"offer_3\",\"offer_4\",\"offer_5\",\"customer_email\",\"row_num\")\n",
    "    write_checkpoint(checkpoint_path, latest_file)\n",
    "    \n",
    "    last_audit_id = df_transactions.select(F.max(\"audit_sur\")).first()[0]\n",
    "    write_last_sur(\"audit_dim\", \"audit_sur\", last_audit_id)\n",
    "    \n",
    "    result = subprocess.run(['hdfs', 'dfs', '-test', '-e', processed_path])\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        write_checkpoint(checkpoint_path, latest_file)\n",
    "        print(f\"File already exists in HDFS: {processed_path}\")\n",
    "        df_transactions.write.parquet(processed_path, mode=\"append\")\n",
    "    else:\n",
    "        write_checkpoint(checkpoint_path, latest_file)\n",
    "        subprocess.run(['hdfs', 'dfs', '-mkdir', '-p', processed_dir])\n",
    "        df_transactions.write.parquet(processed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f85688c3-8457-409b-a1c1-57b36ab47842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of all the columns is : [('transaction_date', 'timestamp'), ('transaction_id', 'string'), ('customer_id', 'int'), ('customer_fname', 'string'), ('customer_lname', 'string'), ('sales_agent_id', 'int'), ('branch_id', 'int'), ('product_id', 'int'), ('product_name', 'string'), ('product_category', 'string'), ('units', 'int'), ('unit_price', 'double'), ('is_online', 'string'), ('payment_method', 'string'), ('shipping_address', 'string'), ('customer_email', 'string'), ('offers', 'string'), ('total_price', 'double'), ('city', 'string'), ('state', 'string'), ('postal_code', 'string'), ('row_num', 'int'), ('audit_sur', 'int'), ('source_file', 'string'), ('created_at', 'timestamp'), ('created_by', 'string'), ('is_valid_email', 'boolean'), ('is_positive_units', 'boolean'), ('is_positive_unit_price', 'boolean'), ('is_valid_transaction', 'boolean')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>offers</th></tr>\n",
       "<tr><td>0</td></tr>\n",
       "<tr><td>10</td></tr>\n",
       "<tr><td>15</td></tr>\n",
       "<tr><td>0</td></tr>\n",
       "<tr><td>5</td></tr>\n",
       "<tr><td>20</td></tr>\n",
       "<tr><td>0</td></tr>\n",
       "<tr><td>0</td></tr>\n",
       "<tr><td>0</td></tr>\n",
       "<tr><td>0</td></tr>\n",
       "<tr><td>0</td></tr>\n",
       "<tr><td>0</td></tr>\n",
       "<tr><td>0</td></tr>\n",
       "<tr><td>20</td></tr>\n",
       "<tr><td>15</td></tr>\n",
       "<tr><td>0</td></tr>\n",
       "<tr><td>0</td></tr>\n",
       "<tr><td>20</td></tr>\n",
       "<tr><td>15</td></tr>\n",
       "<tr><td>0</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+------+\n",
       "|offers|\n",
       "+------+\n",
       "|     0|\n",
       "|    10|\n",
       "|    15|\n",
       "|     0|\n",
       "|     5|\n",
       "|    20|\n",
       "|     0|\n",
       "|     0|\n",
       "|     0|\n",
       "|     0|\n",
       "|     0|\n",
       "|     0|\n",
       "|     0|\n",
       "|    20|\n",
       "|    15|\n",
       "|     0|\n",
       "|     0|\n",
       "|    20|\n",
       "|    15|\n",
       "|     0|\n",
       "+------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Data types of all the columns is : {df_transactions.dtypes}')\n",
    "df_transactions.select('offers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20c811bd-c252-4f02-8b4b-5038c468d386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>date</th></tr>\n",
       "<tr><td>2023520</td></tr>\n",
       "<tr><td>20221025</td></tr>\n",
       "<tr><td>202225</td></tr>\n",
       "<tr><td>20231020</td></tr>\n",
       "<tr><td>20221117</td></tr>\n",
       "<tr><td>2022927</td></tr>\n",
       "<tr><td>2022421</td></tr>\n",
       "<tr><td>2023428</td></tr>\n",
       "<tr><td>202338</td></tr>\n",
       "<tr><td>2023617</td></tr>\n",
       "<tr><td>2022828</td></tr>\n",
       "<tr><td>20231119</td></tr>\n",
       "<tr><td>2022322</td></tr>\n",
       "<tr><td>2022927</td></tr>\n",
       "<tr><td>202243</td></tr>\n",
       "<tr><td>20221214</td></tr>\n",
       "<tr><td>2023414</td></tr>\n",
       "<tr><td>2023119</td></tr>\n",
       "<tr><td>20231014</td></tr>\n",
       "<tr><td>2022324</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------+\n",
       "|    date|\n",
       "+--------+\n",
       "| 2023520|\n",
       "|20221025|\n",
       "|  202225|\n",
       "|20231020|\n",
       "|20221117|\n",
       "| 2022927|\n",
       "| 2022421|\n",
       "| 2023428|\n",
       "|  202338|\n",
       "| 2023617|\n",
       "| 2022828|\n",
       "|20231119|\n",
       "| 2022322|\n",
       "| 2022927|\n",
       "|  202243|\n",
       "|20221214|\n",
       "| 2023414|\n",
       "| 2023119|\n",
       "|20231014|\n",
       "| 2022324|\n",
       "+--------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transactions.select('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dca9327e-a939-435b-8c22-06b369e1bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be514b-744b-456f-a6a0-232074b13506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
