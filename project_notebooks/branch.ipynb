{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "89e8f290-dfad-48a7-8f2c-1da7513c4edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, current_date, col, when,window,to_date,row_number\n",
    "from pyspark.sql.functions import lit, current_date, row_number, col, when, concat_ws, md5\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql import Window\n",
    "import os\n",
    "import subprocess\n",
    "import hashlib\n",
    "from pyspark.sql.functions import to_timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eecca51f-251c-4cbf-86cc-027b0ce26b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c79d0321-3688-4b98-8a27-e7d0c291d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .appName(\"branches\")\\\n",
    "    .config(\"spark.eventLog.logBlockUpdates.enabled\", True)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6c730d6e-99e0-4986-96a8-2f872d0ae3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.repl.eagerEval.maxColWidth\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eaf21ee9-67e9-4ae0-b29d-d8a9f29bbba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_lateset_file(directory):\n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "    list_status = fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(directory))\n",
    "    files = [(file.getPath().toString(), file.getModificationTime()) for file in list_status if file.isFile()]\n",
    "    files.sort(key=lambda x: x[1], reverse=True)\n",
    "    latest_file = files[0][0] if files else None\n",
    "    return latest_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "32edc176-5967-4850-aaa1-c187a90bbc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://localhost:9000/user/branches/branches_SS_raw_2.csv\n"
     ]
    }
   ],
   "source": [
    "latest_file = get_lateset_file('/user/branches')\n",
    "print(latest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dba2177e-de41-4a0c-9003-9837bf54470d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://localhost:9000/user/branches/branches_SS_raw_2.csv\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"/user/checkpoint/checkpoint_branches.txt\"\n",
    "result = subprocess.run(['hdfs', 'dfs', '-test', '-e', checkpoint_path])\n",
    "if result.returncode != 0:\n",
    "    latest_processed_file = \"\"\n",
    "else:\n",
    "    rdd = sc.textFile(\"/user/checkpoint/checkpoint_branches.txt\")\n",
    "    latest_processed_file = rdd.take(rdd.count())[-1]\n",
    "    print(latest_processed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cc5e79f6-ca61-4f7f-967e-1eca2fbe9abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8f803b5e-04ce-4dfc-a88d-cc8e8e06c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"branch_id\", IntegerType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"establish_date\",StringType(), True),\n",
    "    StructField(\"class\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "47ed4a5d-c3c4-4033-a93d-2c7d50e794c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#latest_file = get_lateset_file('/user/branches')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "01664f20-6e46-48bc-aca7-188b3ec3b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_current = spark.read.csv(latest_file, header=True, schema=schema) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6440b93b-5f91-45c8-a4a9-861402cf13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_current.withColumn(\"establish_date\", to_timestamp(col(\"establish_date\"), \"M/d/yyyy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "34742da6-bd7f-48cc-8f9c-a1926741e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bbe75eae-e323-488b-bd08-3407f7b495da",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_old = StructType([\n",
    "    StructField(\"branch_id\", IntegerType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"establish_date\", TimestampType(), True),\n",
    "    StructField(\"class\", StringType(), True),\n",
    "    StructField(\"current_flag\", BooleanType(), True),\n",
    "    StructField(\"effective_date\", TimestampType(), True),\n",
    "    StructField(\"expiration_date\", TimestampType(), True),\n",
    "    StructField(\"sk_branch_id\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "52027d9f-abb4-441a-a046-454933556810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_checkpoint(checkpoint_path, latest_file):\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    result = subprocess.run(['hdfs', 'dfs', '-test', '-e', checkpoint_dir])\n",
    "    if result.returncode != 0:\n",
    "        subprocess.run(['hdfs', 'dfs', '-mkdir', '-p', checkpoint_dir])\n",
    "    \n",
    "    with open('/tmp/checkpoint_tmp.txt', 'w') as f:\n",
    "        f.write(latest_file)\n",
    "    \n",
    "    subprocess.run(['hdfs', 'dfs', '-put', '-f', '/tmp/checkpoint_tmp.txt', checkpoint_path])\n",
    "    os.remove('/tmp/checkpoint_tmp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "802f874e-8a0a-4865-b22c-a74b355a6064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_renamer(df, suffix, append):\n",
    "    if append:\n",
    "        new_column_names = list(map(lambda x: x + suffix, df.columns))\n",
    "    else:\n",
    "        new_column_names = list(map(lambda x: x.replace(suffix, \"\"), df.columns))\n",
    "    return df.toDF(*new_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2be1e6ba-ebf3-457d-b327-6b0a1c4380ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of all the columns is : [('branch_id', 'int'), ('location', 'string'), ('establish_date', 'timestamp'), ('class', 'string')]\n"
     ]
    }
   ],
   "source": [
    "df_current = spark.read.csv(latest_file, header=True, inferSchema=True) \n",
    "print(f'Data types of all the columns is : {df_current.dtypes}')\n",
    "df_current = df_current.withColumn(\"establish_date\", to_timestamp(col(\"establish_date\"), \"M/d/yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9ee882a4-d29c-4366-ad7a-481380753b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+-----+\n",
      "|branch_id|   location|     establish_date|class|\n",
      "+---------+-----------+-------------------+-----+\n",
      "|        1|   New York|2017-01-15 00:00:00|    A|\n",
      "|        2|Los Angeles|2016-07-28 00:00:00|    B|\n",
      "|        3|    Chicago|2015-03-10 00:00:00|    A|\n",
      "|        4|    Houston|2016-11-05 00:00:00|    D|\n",
      "|        5|    Phoenix|2017-09-20 00:00:00|    C|\n",
      "+---------+-----------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_current.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "88a7efc8-6ce5-4a7c-b248-eb2a52952b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash(df, keys_list):\n",
    "    columns = [col(column) for column in keys_list]\n",
    "    if columns:\n",
    "        return df.withColumn(\"hash_md5\", md5(concat_ws(\"\", *columns)))\n",
    "    else:\n",
    "        return df.withColumn(\"hash_md5\", md5(lit(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "160f11d4-4b8c-4f71-be73-e5087e1ed8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+-----+\n",
      "|branch_id|   location|     establish_date|class|\n",
      "+---------+-----------+-------------------+-----+\n",
      "|        1|   New York|2017-01-15 00:00:00|    A|\n",
      "|        2|Los Angeles|2016-07-28 00:00:00|    B|\n",
      "|        3|    Chicago|2015-03-10 00:00:00|    A|\n",
      "|        4|    Houston|2016-11-05 00:00:00|    D|\n",
      "|        5|    Phoenix|2017-09-20 00:00:00|    C|\n",
      "+---------+-----------+-------------------+-----+\n",
      "\n",
      "Data types of all the columns is : [('branch_id', 'int'), ('location', 'string'), ('establish_date', 'timestamp'), ('class', 'string')]\n",
      "+---------+-----------+-------------------+-----+------------+--------------+---------------+------------+\n",
      "|branch_id|   location|     establish_date|class|current_flag|effective_date|expiration_date|sk_branch_id|\n",
      "+---------+-----------+-------------------+-----+------------+--------------+---------------+------------+\n",
      "|        1|   New York|2017-01-15 00:00:00|    A|        true|    2024-07-12|     2024-07-12|           1|\n",
      "|        1|      tanta|2017-01-15 00:00:00|    A|        true|    2024-07-12|     9999-12-31|           2|\n",
      "|        2|Los Angeles|2016-07-28 00:00:00|    B|        true|    2024-07-12|     9999-12-31|           3|\n",
      "|        3|    Chicago|2015-03-10 00:00:00|    A|        true|    2024-07-12|     9999-12-31|           4|\n",
      "|        4|    Houston|2016-11-05 00:00:00|    D|        true|    2024-07-12|     9999-12-31|           5|\n",
      "|        5|    Phoenix|2017-09-20 00:00:00|    C|        true|    2024-07-12|     9999-12-31|           6|\n",
      "|        6|       Alex|2017-09-21 00:00:00|    C|        true|    2024-07-12|     9999-12-31|           7|\n",
      "+---------+-----------+-------------------+-----+------------+--------------+---------------+------------+\n",
      "\n",
      "Data types of all the columns is : [('branch_id', 'int'), ('location', 'string'), ('establish_date', 'timestamp'), ('class', 'string'), ('current_flag', 'boolean'), ('effective_date', 'string'), ('expiration_date', 'string'), ('sk_branch_id', 'int')]\n",
      "+-----------------+----------------+----------------------+-------------+--------------------+----------------------+-----------------------+--------------------+--------------------+-----------------+----------------+----------------------+-------------+--------------------+--------+\n",
      "|branch_id_history|location_history|establish_date_history|class_history|current_flag_history|effective_date_history|expiration_date_history|sk_branch_id_history|    hash_md5_history|branch_id_current|location_current|establish_date_current|class_current|    hash_md5_current|  Action|\n",
      "+-----------------+----------------+----------------------+-------------+--------------------+----------------------+-----------------------+--------------------+--------------------+-----------------+----------------+----------------------+-------------+--------------------+--------+\n",
      "|                1|        New York|   2017-01-15 00:00:00|            A|                true|            2024-07-12|             2024-07-12|                   1|a0834114b722ba675...|                1|        New York|   2017-01-15 00:00:00|            A|a0834114b722ba675...|NOCHANGE|\n",
      "|                1|           tanta|   2017-01-15 00:00:00|            A|                true|            2024-07-12|             9999-12-31|                   2|dba03baae797b2eac...|                1|        New York|   2017-01-15 00:00:00|            A|a0834114b722ba675...|NOCHANGE|\n",
      "|                6|            Alex|   2017-09-21 00:00:00|            C|                true|            2024-07-12|             9999-12-31|                   7|207c55c04eb336bfa...|             null|            null|                  null|         null|                null|  DELETE|\n",
      "|                3|         Chicago|   2015-03-10 00:00:00|            A|                true|            2024-07-12|             9999-12-31|                   4|9c82a6cde8696697d...|                3|         Chicago|   2015-03-10 00:00:00|            A|9c82a6cde8696697d...|NOCHANGE|\n",
      "|                5|         Phoenix|   2017-09-20 00:00:00|            C|                true|            2024-07-12|             9999-12-31|                   6|3923f610fa3157a8d...|                5|         Phoenix|   2017-09-20 00:00:00|            C|3923f610fa3157a8d...|NOCHANGE|\n",
      "|                4|         Houston|   2016-11-05 00:00:00|            D|                true|            2024-07-12|             9999-12-31|                   5|300264d7375cb2d18...|                4|         Houston|   2016-11-05 00:00:00|            D|300264d7375cb2d18...|NOCHANGE|\n",
      "|                2|     Los Angeles|   2016-07-28 00:00:00|            B|                true|            2024-07-12|             9999-12-31|                   3|995091b1bb186255a...|                2|     Los Angeles|   2016-07-28 00:00:00|            B|995091b1bb186255a...|NOCHANGE|\n",
      "+-----------------+----------------+----------------------+-------------+--------------------+----------------------+-----------------------+--------------------+--------------------+-----------------+----------------+----------------------+-------------+--------------------+--------+\n",
      "\n",
      "+---------+-----------+-------------------+-----+------------+--------------+---------------+------------+\n",
      "|branch_id|   location|     establish_date|class|current_flag|effective_date|expiration_date|sk_branch_id|\n",
      "+---------+-----------+-------------------+-----+------------+--------------+---------------+------------+\n",
      "|        1|   New York|2017-01-15 00:00:00|    A|        true|    2024-07-12|     2024-07-12|           1|\n",
      "|        1|      tanta|2017-01-15 00:00:00|    A|        true|    2024-07-12|     9999-12-31|           2|\n",
      "|        3|    Chicago|2015-03-10 00:00:00|    A|        true|    2024-07-12|     9999-12-31|           4|\n",
      "|        5|    Phoenix|2017-09-20 00:00:00|    C|        true|    2024-07-12|     9999-12-31|           6|\n",
      "|        4|    Houston|2016-11-05 00:00:00|    D|        true|    2024-07-12|     9999-12-31|           5|\n",
      "|        2|Los Angeles|2016-07-28 00:00:00|    B|        true|    2024-07-12|     9999-12-31|           3|\n",
      "+---------+-----------+-------------------+-----+------------+--------------+---------------+------------+\n",
      "\n",
      "+---------+--------+--------------+-----+--------------+---------------+------------+------------+\n",
      "|branch_id|location|establish_date|class|effective_date|expiration_date|sk_branch_id|current_flag|\n",
      "+---------+--------+--------------+-----+--------------+---------------+------------+------------+\n",
      "+---------+--------+--------------+-----+--------------+---------------+------------+------------+\n",
      "\n",
      "+---------+--------+-------------------+-----+------------+--------------+---------------+------------+\n",
      "|branch_id|location|     establish_date|class|current_flag|effective_date|expiration_date|sk_branch_id|\n",
      "+---------+--------+-------------------+-----+------------+--------------+---------------+------------+\n",
      "|        6|    Alex|2017-09-21 00:00:00|    C|       false|    2024-07-12|     2024-07-13|           7|\n",
      "+---------+--------+-------------------+-----+------------+--------------+---------------+------------+\n",
      "\n",
      "+---------+--------+--------------+-----+------------+--------------+---------------+------------+\n",
      "|branch_id|location|establish_date|class|current_flag|effective_date|expiration_date|sk_branch_id|\n",
      "+---------+--------+--------------+-----+------------+--------------+---------------+------------+\n",
      "+---------+--------+--------------+-----+------------+--------------+---------------+------------+\n",
      "\n",
      "+---------+-----------+-------------------+-----+------------+--------------+---------------+------------+\n",
      "|branch_id|   location|     establish_date|class|current_flag|effective_date|expiration_date|sk_branch_id|\n",
      "+---------+-----------+-------------------+-----+------------+--------------+---------------+------------+\n",
      "|        1|   New York|2017-01-15 00:00:00|    A|        true|    2024-07-12|     2024-07-12|           1|\n",
      "|        1|      tanta|2017-01-15 00:00:00|    A|        true|    2024-07-12|     9999-12-31|           2|\n",
      "|        3|    Chicago|2015-03-10 00:00:00|    A|        true|    2024-07-12|     9999-12-31|           4|\n",
      "|        5|    Phoenix|2017-09-20 00:00:00|    C|        true|    2024-07-12|     9999-12-31|           6|\n",
      "|        4|    Houston|2016-11-05 00:00:00|    D|        true|    2024-07-12|     9999-12-31|           5|\n",
      "|        2|Los Angeles|2016-07-28 00:00:00|    B|        true|    2024-07-12|     9999-12-31|           3|\n",
      "|        6|       Alex|2017-09-21 00:00:00|    C|       false|    2024-07-12|     2024-07-13|           7|\n",
      "+---------+-----------+-------------------+-----+------------+--------------+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_path = \"/user/silver/branches/branch_dim.parquet\"\n",
    "scd_path = \"/user/silver/branches/scd2\"\n",
    "processed_dir = \"/user/silver/branches\"\n",
    "checkpoint_path = \"/user/checkpoint/checkpoint_branches.txt\"\n",
    "\n",
    "EOW_DATE = \"9999-12-31\"\n",
    "DATE_FORMAT = \"yyyy-MM-dd\"\n",
    "type2_cols = [\"location\", \"establish_date\", \"class\"]\n",
    "latest_file = get_lateset_file('/user/branches')\n",
    "\n",
    "if latest_processed_file == latest_file:\n",
    "    print(f\"File already processed before: {latest_file}\")\n",
    "    spark.stop()\n",
    "else:\n",
    "    result = subprocess.run(['hdfs', 'dfs', '-test', '-e', processed_dir])\n",
    "    if result.returncode == 0:\n",
    "        \n",
    "        df_current = spark.read.csv(latest_file, header=True, inferSchema=True) \n",
    "        df_current = df_current.withColumn(\"establish_date\", to_timestamp(col(\"establish_date\"), \"M/d/yyyy\"))\n",
    "        df_current.show()\n",
    "        print(f'Data types of all the columns is : {df_current.dtypes}')\n",
    "        \n",
    "        window_spec = Window.orderBy(\"branch_id\")\n",
    "        \n",
    "        result = subprocess.run(['hdfs', 'dfs', '-test', '-e', scd_path])\n",
    "        if result.returncode == 0:\n",
    "            df_history = spark.read.parquet(\"/user/silver/branches/scd2/*.parquet\")\\\n",
    "                        .withColumn(\"current_flag\", lit(True)) \\\n",
    "                        .withColumn(\"sk_branch_id\", row_number().over(window_spec))\n",
    "        else:\n",
    "            df_history = spark.read.parquet(\"/user/silver/branches/*.parquet\")\\\n",
    "            .withColumn(\"current_flag\", lit(True)) \\\n",
    "            .withColumn(\"sk_branch_id\", row_number().over(window_spec))\n",
    "        \n",
    "        df_history.show()\n",
    "        \n",
    "        print(f'Data types of all the columns is : {df_history.dtypes}')\n",
    "        \n",
    "        df_history_open = df_history.where(col(\"current_flag\") == True)\n",
    "        df_history_closed = df_history.where(col(\"current_flag\") == False)\n",
    "        \n",
    "        max_sk = df_history_open.agg({\"sk_branch_id\": \"max\"}).collect()[0][0]\n",
    "        \n",
    "        df_history_open_hash = column_renamer(get_hash(df_history_open, type2_cols), suffix=\"_history\", append=True)\n",
    "        df_current_hash = column_renamer(get_hash(df_current, type2_cols), suffix=\"_current\", append=True)\n",
    "        \n",
    "        \n",
    "                        \n",
    "                        \n",
    "        df_merged = df_history_open_hash \\\n",
    "            .join(df_current_hash, col(\"branch_id_current\") == col(\"branch_id_history\"), how=\"full_outer\") \\\n",
    "            .withColumn(\"Action\", \n",
    "                when(col(\"hash_md5_current\") == col(\"hash_md5_history\"), 'NOCHANGE')\n",
    "                .when(col(\"branch_id_current\").isNull(), 'DELETE')\n",
    "                .when(col(\"branch_id_history\").isNull(), 'INSERT')\n",
    "                .when(col(\"branch_id_history\") == col(\"branch_id_current\"), 'NOCHANGE')  # Corrected this line\n",
    "                .otherwise('UPDATE'))\n",
    "\n",
    "        \n",
    "        df_merged.show()\n",
    "        \n",
    "        \n",
    "        window_spec  = Window.orderBy(\"branch_id\")\n",
    "\n",
    "\n",
    "        df_nochange = column_renamer(df_merged.filter(col(\"action\") == 'NOCHANGE'), suffix=\"_history\", append=False)\\\n",
    "                        .select(df_history_open.columns)\n",
    "\n",
    "        df_nochange.show()\n",
    "        \n",
    "        df_insert = column_renamer(df_merged.filter(col(\"action\") == 'INSERT'), suffix=\"_current\", append=False)\\\n",
    "                .select(df_current.columns)\\\n",
    "                .withColumn(\"effective_date\",date_format(current_date(),DATE_FORMAT))\\\n",
    "                .withColumn(\"expiration_date\",date_format(lit(EOW_DATE),DATE_FORMAT))\\\n",
    "                .withColumn(\"row_number\",row_number().over(window_spec))\\\n",
    "                .withColumn(\"sk_branch_id\",col(\"row_number\")+ max_sk)\\\n",
    "                .withColumn(\"current_flag\", lit(True))\\\n",
    "                .drop(\"row_number\")\n",
    "\n",
    "        df_insert.show()\n",
    "        \n",
    "        max_sk_i = df_insert.agg({\"sk_branch_id\": \"max\"}).collect()[0][0]\n",
    "\n",
    "        df_deleted = column_renamer(df_merged.filter(col(\"action\") == 'DELETE'), suffix=\"_history\", append=False)\\\n",
    "                .select(df_history_open.columns)\\\n",
    "                .withColumn(\"expiration_date\", date_format(current_date(),DATE_FORMAT))\\\n",
    "                .withColumn(\"current_flag\", lit(False))\n",
    "\n",
    "        df_deleted.show()\n",
    "        \n",
    "        \n",
    "        df_update = column_renamer(df_merged.filter(col(\"action\") == 'UPDATE'), suffix=\"_history\", append=False)\\\n",
    "                .select(df_history_open.columns)\\\n",
    "                .withColumn(\"expiration_date\", date_format(current_date(),DATE_FORMAT))\\\n",
    "                .withColumn(\"current_flag\", lit(False))\\\n",
    "            .unionByName(\n",
    "            column_renamer(df_merged.filter(col(\"action\") == 'UPDATE'), suffix=\"_current\", append=False)\\\n",
    "                .select(df_current.columns)\\\n",
    "                .withColumn(\"effective_date\",date_format(current_date(),DATE_FORMAT))\\\n",
    "                .withColumn(\"expiration_date\",date_format(lit(EOW_DATE),DATE_FORMAT))\\\n",
    "                .withColumn(\"row_number\",row_number().over(window_spec))\\\n",
    "                .withColumn(\"sk_branch_id\",col(\"row_number\")+ max_sk_i)\\\n",
    "                .withColumn(\"current_flag\", lit(True))\\\n",
    "                .drop(\"row_number\")\n",
    "                )\n",
    "        df_update.show()\n",
    "        \n",
    "        df_final = df_history_closed\\\n",
    "            .unionByName(df_nochange)\\\n",
    "            .unionByName(df_insert)\\\n",
    "            .unionByName(df_deleted)\\\n",
    "            .unionByName(df_update)\n",
    "        df_final.show()\n",
    "        #df_final.write.parquet(scd_path)\n",
    "        #write_checkpoint(checkpoint_path, latest_file)\n",
    "    else:\n",
    "        df_new = spark.read.csv(latest_file, header=True, inferSchema=True)\\\n",
    "            .withColumn(\"current_flag\", lit(True)) \\\n",
    "            .withColumn(\"effective_date\", current_date()) \\\n",
    "            .withColumn(\"expiration_date\", lit(EOW_DATE).cast(\"date\"))\n",
    "        \n",
    "        write_checkpoint(checkpoint_path, latest_file)\n",
    "        print(f\"New file processed: {latest_file}\")\n",
    "        df_new.write.parquet(processed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "81235bc7-5e65-4e66-bf30-d57d9c8b29bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = spark.read.parquet(\"/user/silver/branches/branch_dim.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3e04c006-ebb6-4890-bc2a-8f420537e150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+-----+------------+--------------+---------------+------------+\n",
      "|branch_id|   location|     establish_date|class|current_flag|effective_date|expiration_date|sk_branch_id|\n",
      "+---------+-----------+-------------------+-----+------------+--------------+---------------+------------+\n",
      "|        2|Los Angeles|2016-07-28 00:00:00|    B|        true|    2024-07-12|     9999-12-31|           2|\n",
      "|        1|   New York|2017-01-15 00:00:00|    A|       false|    2024-07-12|     2024-07-12|           1|\n",
      "|        3|    Chicago|2015-03-10 00:00:00|    A|        true|    2024-07-12|     9999-12-31|           3|\n",
      "|        5|    Phoenix|2017-09-20 00:00:00|    C|        true|    2024-07-12|     9999-12-31|           5|\n",
      "|        4|    Houston|2016-11-05 00:00:00|    D|        true|    2024-07-12|     9999-12-31|           4|\n",
      "|        1|      tanta|2017-01-15 00:00:00|    A|        true|    2024-07-12|     9999-12-31|           7|\n",
      "|        6|       Alex|2017-09-21 00:00:00|    C|        true|    2024-07-12|     9999-12-31|           6|\n",
      "+---------+-----------+-------------------+-----+------------+--------------+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8308eb31-2c80-4133-a4e7-9770f08735a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_current = spark.read.csv(latest_file, header=True, schema=schema) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09f7558f-60f6-4d46-9d12-8be752904d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of all the columns is : [('branch_id', 'int'), ('location', 'string'), ('establish_date', 'string'), ('class', 'string')]\n"
     ]
    }
   ],
   "source": [
    "print(f'Data types of all the columns is : {df_current.dtypes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "95a2af47-c240-4056-9b17-a420d943bc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of all the columns is : [('branch_id', 'int'), ('location', 'string'), ('establish_date', 'timestamp'), ('class', 'string'), ('current_flag', 'boolean'), ('effective_date', 'date'), ('expiration_date', 'date')]\n"
     ]
    }
   ],
   "source": [
    "print(f'Data types of all the columns is : {df_new.dtypes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8cdd8f0-92db-4822-8c30-bad6a45960f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+-----+----------+-------------------+-------------------+\n",
      "|branch_id|   location|     establish_date|class|is_current|         start_date|           end_date|\n",
      "+---------+-----------+-------------------+-----+----------+-------------------+-------------------+\n",
      "|        1|   New York|2017-01-15 00:00:00|    A|      true|2024-07-11 00:00:00|9999-12-31 00:00:00|\n",
      "|        2|Los Angeles|2016-07-28 00:00:00|    B|      true|2024-07-11 00:00:00|9999-12-31 00:00:00|\n",
      "|        3|    Chicago|2015-03-10 00:00:00|    A|      true|2024-07-11 00:00:00|9999-12-31 00:00:00|\n",
      "|        4|    Houston|2016-11-05 00:00:00|    D|      true|2024-07-11 00:00:00|9999-12-31 00:00:00|\n",
      "|        5|    Phoenix|2017-09-20 00:00:00|    C|      true|2024-07-11 00:00:00|9999-12-31 00:00:00|\n",
      "+---------+-----------+-------------------+-----+----------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f79ec5f-5b5e-4cc3-8d97-c6d222eb35fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of all the columns is : [('branch_id', 'int'), ('location', 'string'), ('establish_date', 'date'), ('class', 'string'), ('is_current', 'boolean'), ('start_date', 'date'), ('end_date', 'date'), ('sk_branch_id', 'int')]\n"
     ]
    }
   ],
   "source": [
    "print(f'Data types of all the columns is : {df_final.dtypes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bff86fc7-0580-424e-a33b-10f620248a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------------------------+---+----+-------------------+-------------------+\n",
      "|  1|   New York|2017-01-15T00:00:00.000Z|  A|true|         2024-07-11|         9999-12-31|\n",
      "+---+-----------+------------------------+---+----+-------------------+-------------------+\n",
      "|  2|Los Angeles|     2016-07-28 00:00:00|  B|true|2024-07-11 00:00:00|9999-12-31 00:00:00|\n",
      "|  3|    Chicago|     2015-03-10 00:00:00|  A|true|2024-07-11 00:00:00|9999-12-31 00:00:00|\n",
      "|  4|    Houston|     2016-11-05 00:00:00|  D|true|2024-07-11 00:00:00|9999-12-31 00:00:00|\n",
      "|  5|    Phoenix|     2017-09-20 00:00:00|  C|true|2024-07-11 00:00:00|9999-12-31 00:00:00|\n",
      "+---+-----------+------------------------+---+----+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_existing.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dca9327e-a939-435b-8c22-06b369e1bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5bcbff-8d64-4443-8451-f0ebc878a3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
